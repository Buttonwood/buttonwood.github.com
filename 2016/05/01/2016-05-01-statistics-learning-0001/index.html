<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  
  <title>统计学习第一章 | Buttonwood</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  
    <meta name="author" content="Buttonwood">
  
  
    <meta name="description" content="第一章 基础1. 一般步骤:
得到一个有限训练集(Date Set)
确定假设空间(学习模型的集合Model Set)
确定模型的选择准则(学习策略Strategy)
实现求解最优化模型的算法(Algotithm)
通过学习方法选择最有模型
利用学习的最优模型对新数据集进行预测或分析



2. 模型2.1 非概率模型
决策函数空间:


\mathscr{F}=\\{f|Y=f(X)\\} \t">
  
  <meta name="description" content="第一章 基础1. 一般步骤: 得到一个有限训练集(Date Set) 确定假设空间(学习模型的集合Model Set) 确定模型的选择准则(学习策略Strategy) 实现求解最优化模型的算法(Algotithm) 通过学习方法选择最有模型 利用学习的最优模型对新数据集进行预测或分析    2. 模型2.1 非概率模型 决策函数空间:   \mathscr{F}=\\{f|Y=f(X)\\} \t">
<meta name="keywords" content="ML,DM">
<meta property="og:type" content="article">
<meta property="og:title" content="统计学习第一章">
<meta property="og:url" content="http://buttonwood.github.io/2016/05/01/2016-05-01-statistics-learning-0001/index.html">
<meta property="og:site_name" content="Buttonwood">
<meta property="og:description" content="第一章 基础1. 一般步骤: 得到一个有限训练集(Date Set) 确定假设空间(学习模型的集合Model Set) 确定模型的选择准则(学习策略Strategy) 实现求解最优化模型的算法(Algotithm) 通过学习方法选择最有模型 利用学习的最优模型对新数据集进行预测或分析    2. 模型2.1 非概率模型 决策函数空间:   \mathscr{F}=\\{f|Y=f(X)\\} \t">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://img.hb.aicdn.com/d1912cce39b6c090ab97c438335f00334cf5259313637-9BC80a_fw658">
<meta property="og:image" content="http://img.hb.aicdn.com/8b4c60e1dffa92421d1a92d81eb627c30d3751d0f8ed-JaCpaw_fw658">
<meta property="og:image" content="http://blogs.sas.com/content/subconsciousmusings/files/2017/04/CheatSheet.png">
<meta property="og:updated_time" content="2018-07-10T14:24:59.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="统计学习第一章">
<meta name="twitter:description" content="第一章 基础1. 一般步骤: 得到一个有限训练集(Date Set) 确定假设空间(学习模型的集合Model Set) 确定模型的选择准则(学习策略Strategy) 实现求解最优化模型的算法(Algotithm) 通过学习方法选择最有模型 利用学习的最优模型对新数据集进行预测或分析    2. 模型2.1 非概率模型 决策函数空间:   \mathscr{F}=\\{f|Y=f(X)\\} \t">
<meta name="twitter:image" content="http://img.hb.aicdn.com/d1912cce39b6c090ab97c438335f00334cf5259313637-9BC80a_fw658">
  
    <link rel="alternate" href="/atom.xml" title="Buttonwood" type="application/atom+xml">
  
  
    <link rel="icon" type="image/x-icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="wrapper">
    <header id="header">
  <div class="title">
    <h1><a href="/">Buttonwood</a></h1>
    <p><a href="/">Being A Coding Poet. Less Talk, and More Move!</a></p>
  </div>
  <nav class="nav">
    <ul>
      
        <li><a href="/about/">About</a></li>
      
        <li><a href="/archives/">Archives</a></li>
      
        <li><a href="/links/">Links</a></li>
      
      
        <li><a href="/atom.xml">RSS</a></li>
      
    </ul>
    <div class="clearfix"></div>
  </nav>
  <div class="clearfix"></div>
</header>
    <div class="content"><article class="post">
  <header>
    
      <div class="icon"></div>
      <a href="/2016/05/01/2016-05-01-statistics-learning-0001/">
  <time datetime="2016-05-01T13:51:00.000Z">
    May 1 2016
  </time>
</a>
    
    
  
    <h1 class="title">统计学习第一章</h1>
  

  </header>
  
  <div class="entry">
    
      <h3 id="第一章-基础"><a href="#第一章-基础" class="headerlink" title="第一章 基础"></a>第一章 基础</h3><h4 id="1-一般步骤"><a href="#1-一般步骤" class="headerlink" title="1. 一般步骤:"></a>1. 一般步骤:</h4><ol>
<li>得到一个有限训练集(Date Set)</li>
<li>确定假设空间(学习模型的集合Model Set)</li>
<li>确定模型的选择准则(学习策略Strategy)</li>
<li>实现求解最优化模型的算法(Algotithm)</li>
<li>通过学习方法选择最有模型</li>
<li>利用学习的最优模型对新数据集进行预测或分析</li>
</ol>
<p><img src="http://img.hb.aicdn.com/d1912cce39b6c090ab97c438335f00334cf5259313637-9BC80a_fw658" alt=""></p>
<p><img src="http://img.hb.aicdn.com/8b4c60e1dffa92421d1a92d81eb627c30d3751d0f8ed-JaCpaw_fw658" alt=""></p>
<h4 id="2-模型"><a href="#2-模型" class="headerlink" title="2. 模型"></a>2. 模型</h4><h5 id="2-1-非概率模型"><a href="#2-1-非概率模型" class="headerlink" title="2.1 非概率模型"></a>2.1 非概率模型</h5><ul>
<li>决策函数空间:</li>
</ul>
<script type="math/tex; mode=display">
\mathscr{F}=\\{f|Y=f(X)\\} \tag{1}</script><ul>
<li>参数向量$ \theta \in \mathbf{R}^n $决定的函数族:</li>
</ul>
<script type="math/tex; mode=display">
\mathscr{F}=\\{f|Y=f_{\theta}(X),\theta \in \mathbf{R}^n\\} \tag{2}</script><h5 id="2-2-概率模型"><a href="#2-2-概率模型" class="headerlink" title="2.2 概率模型"></a>2.2 概率模型</h5><ul>
<li><p>条件概率集合:</p>
<script type="math/tex; mode=display">
\mathscr{F}=\\{P|P(Y|X)\\} \tag{3}</script></li>
<li><p>参数向量$$$\theta \in \mathbf{R}^n$$$决定的条件概率分布族:</p>
</li>
</ul>
<script type="math/tex; mode=display">
\mathscr{F}=\\{P|P_{\theta}(Y|X),\theta \in \mathbf{R}^n\\} \tag{4}</script><h4 id="3-策略"><a href="#3-策略" class="headerlink" title="3. 策略"></a>3. 策略</h4><h5 id="3-1-损失函数或风险函数"><a href="#3-1-损失函数或风险函数" class="headerlink" title="3.1 损失函数或风险函数"></a>3.1 损失函数或风险函数</h5><ul>
<li>$ \text{0-1}\ Loss\ Function $</li>
</ul>
<p>\[<br>    L(Y,f(X)) =<br>    \begin{cases}<br>        1,  &amp;\text{$Y \ne f(X)$}\\\<br>        0, &amp;\text{$Y=f(X)$}<br>    \end{cases} \tag{5}<br>\]</p>
<ul>
<li>$ Quadratic\ Loss\ Function $</li>
</ul>
<p>\[<br>    L(Y,f(X)) = (Y-f(X))^2 \tag{6}<br>\]</p>
<ul>
<li>$ Absolute\ Loss\ Function $</li>
</ul>
<p>\[<br>    L(Y,f(X)) = |Y-f(X)| \tag{7}<br>\]</p>
<ul>
<li>$ Logarithmic\ Loss\ Function\ or\ \text{log-likelihood}\ Loss\ Function $</li>
</ul>
<p>\[<br>    L(Y,P(Y|X)) = -logP(Y|X) \tag{8}<br>\]</p>
<ul>
<li>$ Risk\ Function\ or\ Expected\ Loss $ </li>
</ul>
<p>\[<br>R_{exp}(f)=E_p[L(Y,P(Y|X))]= \int_\{x \times y\} L(y,f(x))P(x,y)dxdy \tag{9}<br>\]</p>
<p>联合分布$ P(X,Y) $未知, $ R_{exp}(f) $不能直接计算.</p>
<ul>
<li>$ Empirical\ Risk\ or\ Empirical\ Loss $ </li>
</ul>
<p>\[<br>R_{emp}(f)=\frac{1}{N} \sum_{i=1}^{N}L(y_i,f(x_i)) \tag{10}<br>\]</p>
<p>期望风险$ R_{exp}(f) $是模型关于联合分布的期望损失, 经验风险$ R_{emp}(f) $是模型关于训练样本集的平均损失.</p>
<p>根据大数定律 $ \lim_{N\to\infty}R_{emp}=R_{exp} $.可用经验风险估计期望风险.</p>
<ul>
<li>经验风险最小化:</li>
</ul>
<p>\[<br>min_{f \in \mathscr{F}}R_{emp}(f)=min_{f \in \mathscr{F}}\frac{1}{N} \sum_{i=1}^{N}L(y_i,f(x_i)) \tag{11}<br>\]</p>
<p>当样本容量足够大,经验风险最小化能保证很好的学习效果,如<strong>MLE</strong>$ 极大似然估计(Maximum\ Likelihood\ Estimation) $.</p>
<p>当模型是条件概率分布,损失函数是对数函数时,经验风险最小化(<strong>EMP</strong>)等价于极大似然估计(<strong>MLE</strong>).</p>
<p>但是,当样本容量很小时,会产生过拟合(<strong>over-fitting</strong>)现象.于是,为了防止过拟合,可采用结构风险最小化(<strong>Structural Risk Minimization,SRM</strong>).</p>
<ul>
<li>$ Structural\ Risk\ or\ Structural\ Loss $ </li>
</ul>
<p>\[<br>R_{srm}(f)=\frac{1}{N} \sum_{i=1}^{N}L(y_i,f(x_i))  + \lambda J(f) \tag{12}<br>\]</p>
<p>结构风险在经验风险上加上表示模型复杂度的正则化项(<strong>regularizer</strong>)或罚项(<strong>penalty term</strong>).</p>
<p>结构风险最小化(<strong>SRM</strong>)等价于正则化(<strong>Regularization</strong>).</p>
<p>$ J(f) $为模型的复杂度,是定义在假设空间$ \mathscr{F} $上的泛函.模型越复杂,复杂度越大,复杂度一定意义上表示了对复杂模型的惩罚.</p>
<ul>
<li>结构风险最小化:</li>
</ul>
<p>\[<br>min_{f \in \mathscr{F}}R_{srm}(f)=min_{f \in \mathscr{F}}\frac{1}{N} \sum_{i=1}^{N}L(y_i,f(x_i))  + \lambda J(f) \tag{13}<br>\]</p>
<p>如贝叶斯估计中的最大后验概率估计(<strong>Maximum Posterior Probability,MAP</strong>)就是结构风险最小化的一个例子.</p>
<p>当模型是条件概率分布,损失函数是对数函数,模型复杂度由模型的先验概率表示时,结构风险最小化(<strong>SRM</strong>)等价于最大后验概率估计(<strong>MAP</strong>).</p>
<h4 id="4-算法"><a href="#4-算法" class="headerlink" title="4. 算法"></a>4. 算法</h4><p>统计学习问题归结为最优化问题,统计学习的算法成为求解最优化问题的算法.</p>
<ol>
<li>有解析式,解析求解</li>
<li>无解析式,数值计算</li>
</ol>
<h4 id="5-模型评估与模型选择"><a href="#5-模型评估与模型选择" class="headerlink" title="5. 模型评估与模型选择"></a>5. 模型评估与模型选择</h4><h5 id="5-1-训练误差与测试误差"><a href="#5-1-训练误差与测试误差" class="headerlink" title="5.1 训练误差与测试误差"></a>5.1 训练误差与测试误差</h5><ol>
<li>基于损失函数的训练误差(training error).训练误差的大小,对判断给定的问题是不是一个容易学习的问题是有意义的,但本质上不重要.</li>
<li>基于模型的测试误差(test error).测试误差反映了学习方法对未知的测试数据集的预测能力,测试误差小的方法通常有更好的预测能力(也称泛化能力generralization ability),是更有效的方法.</li>
</ol>
<p><strong><em>Remarks:</em></strong>学习时具体采用的损失函数未必是评估时使用的损失函数.当然,让二者一致是比较理想的.</p>
<ul>
<li>训练误差是学习到的模型$ \widehat{Y}=\widehat{f}(X) $ 关于训练集的平均损失:</li>
</ul>
<p>\[<br>R_{emp}(\widehat{f})=\frac{1}{N} \sum_{i=1}^{N}L(y_i,\widehat{f}(x_i)) \tag{14}<br>\]</p>
<ul>
<li>测试误差是学习到的模型$ \widehat{Y}=\widehat{f}(X) $ 关于测试集的平均损失:</li>
</ul>
<p>\[<br>e_{test}(\widehat{f})=\frac{1}{M} \sum_{i=1}^{M}L(y_i,\widehat{f}(x_i)) \tag{15}<br>\]</p>
<p><strong>e.g.</strong> 当损失函数是0/1损失时,测试误差就变成了常见的测试数据集上的误差率(error rate)        </p>
<p>\[<br>e_{test}(\widehat{f})=\frac{1}{M} \sum_{i=1}^{M}I(y_i \ne \widehat{f}(x_i)) \tag{16}<br>\]    </p>
<p>相应的,常见测试数据集上的准确率(accuracy):</p>
<p>\[<br>    r_{test}(\widehat{f})=\frac{1}{M} \sum_{i=1}^{M}I(y_i = \widehat{f}(x_i)) \tag{16}<br>\]    </p>
<p>显然, \[ e_{test} + r_{test} = 1\]</p>
<h5 id="5-2-过拟合与模型选择"><a href="#5-2-过拟合与模型选择" class="headerlink" title="5.2 过拟合与模型选择"></a>5.2 过拟合与模型选择</h5><p><strong>过拟合(Over-fitting):</strong>学习时选择的模型所包含的参数过多,以致于出现模型对已知数据(训练集)预测很好,对未知数据预测很差的现象.</p>
<p>多项式拟合的例子(P27)说明:当模型的复杂度增大时,训练误差会逐渐减小并趋向0;而测试误差会先减小后增大.当选择的模型复杂度过大,过拟合的现象就会发生.</p>
<h5 id="5-3-正则化与交叉验证"><a href="#5-3-正则化与交叉验证" class="headerlink" title="5.3 正则化与交叉验证"></a>5.3 正则化与交叉验证</h5><ul>
<li>正则化(Regularization):正则化是结构风险最小化策略的实现,是经验风险加上一个正则化项或罚项,正则化项一般是模型复杂度的单调递增函数,比如模型参数向量的范数.</li>
</ul>
<p><strong>e.g.</strong>:回归问题中,损失函数是平方损失,正则化项可以是参数向量的$ L_2 $范数.</p>
<p>\[L(w)=\frac{1}{N} \sum_{i=1}^{N}(f(x_i;w)-y_i))^2  + \frac{\lambda}{2}||w||_1  \tag{17}\]</p>
<ul>
<li>能够很好地解释已知数据并且十分简单才是好地模型.</li>
<li>从贝叶斯估计的角度来看,正则项对英语模型的先验概率,可以假设复杂的模型有较大的先验概率.</li>
</ul>
<ul>
<li>交叉验证(Cross Validation):</li>
</ul>
<p>数据充足时,随机分层:</p>
<ul>
<li>training set</li>
<li>validation set</li>
<li>test set</li>
</ul>
<p>数据不足时,交叉验证:重复使用数据,将给定的数据进行切分,将切分的数据合为训练集和测试集,在此基础上反复进行训练.</p>
<ol>
<li>简单交叉验证:(70%training set,30%test set)</li>
<li>S-fold cross validation:随机分成S个互不相交的大小相同的子集,S-1个用于训练,余下测试;对可能的S种组合重复选择,选出S次评测种平均测试误差最小的.</li>
<li>留一交叉验证(leave-one-out cross validation):S=N,往往数据缺乏的情况下使用.</li>
</ol>
<h4 id="6-泛化能力"><a href="#6-泛化能力" class="headerlink" title="6.泛化能力"></a>6.泛化能力</h4><p>Generalization Ability:对未知数据的预测能力.一般通过测试误差来评价.</p>
<h5 id="6-1-泛化误差-generalization-error"><a href="#6-1-泛化误差-generalization-error" class="headerlink" title="6.1 泛化误差(generalization error)"></a>6.1 泛化误差(generalization error)</h5><p>事实上,范化误差就是所学习到的模型的期望风险.<br>\[<br>R_{exp}(\widehat{f})=E_p[L(Y,\widehat{f}(X))]= \int_\{x \times y\} L(y,\widehat{f}(x))P(x,y)dxdy \tag{18}<br>\]</p>
<h5 id="6-2-泛化误差上界-generalization-error-bound"><a href="#6-2-泛化误差上界-generalization-error-bound" class="headerlink" title="6.2 泛化误差上界(generalization error bound)"></a>6.2 泛化误差上界(generalization error bound)</h5><ul>
<li>性质<ul>
<li>它是样本容量的函数</li>
<li>它是假设空间的函数</li>
</ul>
</li>
</ul>
<p><strong>e.g.</strong>:二分类问题的泛化误差上界<br>    <strong>Hoeffding不等式</strong>: $ S_n=\sum_{i=1}^{n}X_i $是独立随机变量$ X_1,X_2,..,X_n $之和,$ X_i \in [a_i,b_i] $,则对任意$ t&gt;0 $,以下不等式成立:</p>
<p>\[<br>P(S_n-E[S_n] \geqslant t) \leqslant \exp{\frac{-2t^2}{\sum_{i=1}^{n}(b_i-a_i)^2}} \tag{19}<br>\]</p>
<p>\[<br>P(E[S_n] - S_n \leqslant t) \geqslant \exp{\frac{-2t^2}{\sum_{i=1}^{n}(b_i-a_i)^2}} \tag{20}<br>\]</p>
<h4 id="7-生成模型与判别模型"><a href="#7-生成模型与判别模型" class="headerlink" title="7. 生成模型与判别模型"></a>7. 生成模型与判别模型</h4><ol>
<li>监督学习的任务就是学习一个模型决策函数$ Y=f(X) $或条件概率分布$ P(Y|X) $</li>
<li>监督学习的方法可以分为生成方法(generative approach)和判别方法(discriminative approach)</li>
</ol>
<h5 id="7-1-生成模型-Generative-Model"><a href="#7-1-生成模型-Generative-Model" class="headerlink" title="7.1 生成模型(Generative Model)"></a>7.1 生成模型(Generative Model)</h5><p>由数据学习联合概率分布$ P(X,Y) $,然后求出条件概率分布$ P(Y|X) $.即给定X,输出Y.</p>
<p>\[<br>P(Y|X)=\frac{P(X,Y)}{P(X)}<br>\]</p>
<p><strong>e.g.</strong>:</p>
<ul>
<li>朴素贝叶斯</li>
<li>隐马尔可夫链HMM</li>
</ul>
<p><strong>特点</strong>:</p>
<ul>
<li>可还原出联合概率分布$ P(X,Y) $</li>
<li>收敛速度更快</li>
<li>存在隐变量时仍适用    </li>
</ul>
<h5 id="7-2-判别模型-Discriminative-Model"><a href="#7-2-判别模型-Discriminative-Model" class="headerlink" title="7.2 判别模型(Discriminative Model)"></a>7.2 判别模型(Discriminative Model)</h5><p>直接学习决策函数$ Y=f(X) $或条件概率分布$ P(Y|X) $.对应给定的输入X,应该预测什么样的输出Y</p>
<p><strong>e.g.</strong>:</p>
<ul>
<li>k近邻法</li>
<li>感知机</li>
<li>决策树</li>
<li>逻辑回归</li>
<li>最大熵</li>
<li>支持向量机SVM</li>
<li>提升方法</li>
<li>条件随机场</li>
</ul>
<p><strong>特点</strong>:</p>
<ul>
<li>直接面对预测,往往准确率更高</li>
<li>可以对数据进行各种程度的抽象,定义特征并适用,简化学习问题</li>
</ul>
<h4 id="8-分类问题"><a href="#8-分类问题" class="headerlink" title="8.分类问题"></a>8.分类问题</h4><p>输出变量Y是有限个离散值时,监督学习的预测问题变成分类问题.输入变量X可以是离散的,也可以是连续的.</p>
<ul>
<li>classifier</li>
<li>prediction-&gt;classification</li>
<li>class</li>
</ul>
<h5 id="8-1-二分类问题"><a href="#8-1-二分类问题" class="headerlink" title="8.1 二分类问题"></a>8.1 二分类问题</h5><ul>
<li>$ TP $ 将正类预测为正类</li>
<li>$ FN $ 将正类预测为负类</li>
<li>$ FP $ 将负类预测为正类</li>
<li>$ TN $ 将负类预测为负类</li>
<li>精确率(precision) \[P=\frac{TP}{TP+FP}\]</li>
<li>召回率(recall) \[R=\frac{TP}{TP+FN}\]</li>
<li>$ F_1 $值:精确率和召回率的调和平均 \[F_1=\frac{2TP}{2TP+FP+FN}\]</li>
</ul>
<h5 id="8-2-标注问题-tagging"><a href="#8-2-标注问题-tagging" class="headerlink" title="8.2 标注问题(tagging)"></a>8.2 标注问题(tagging)</h5><p>输入一个观测序列,输出一个标记序列或状态序列,是分类问题的一个推广.</p>
<ul>
<li>HMM</li>
<li>SGD</li>
<li>信息抽取,NLP</li>
</ul>
<h4 id="9-回归问题-Regression"><a href="#9-回归问题-Regression" class="headerlink" title="9.回归问题(Regression)"></a>9.回归问题(Regression)</h4><p>用于预测输入变量X(自变量)和输出变量Y(因变量)之间的关系,等价于函数拟合.</p>
<ul>
<li>一元回归和多元回归</li>
<li>线性回归和非线性回归</li>
<li>平方损失函数-&gt;最小而乘法(Least Squares)</li>
<li>应用于市场预测／产品质量管理／客户满意度／投资风险分析等</li>
</ul>
<h3 id="Roadmap"><a href="#Roadmap" class="headerlink" title="Roadmap"></a>Roadmap</h3><p><img src="http://blogs.sas.com/content/subconsciousmusings/files/2017/04/CheatSheet.png" alt=""></p>

    
  </div>
  <footer>
    
      
  <div class="categories">
    <a class="categories-link" href="/categories/ML/">ML</a>
  </div>

      
  <div class="tags">
    <a class="tags-link" href="/tags/DM/">DM</a>, <a class="tags-link" href="/tags/ML/">ML</a>
  </div>

    
    <div class="clearfix"></div>
  </footer>
</article>


<section id="comment"> 
    <h1 class="title">Comments</h1>
    <div class="ds-thread" data-title="统计学习第一章"></div> 
</section>

</div>
  </div>
  <footer id="footer"><div class="copyright">
  
  &copy; 2018 <a href="/">Buttonwood</a>
  
</div>
<div class="theme-copyright">
  Theme by <a href="https://github.com/orderedlist" target="_blank">orderedlist</a>
   | 
  Redesign by <a href="http://heroicyang.com/" target="_blank">Heroic Yang</a>
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js"></script>
<script src="/js/scale.fix.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
  var duoshuoQuery = { short_name: 'buttonwood' };
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';
    ds.async = true;
    ds.src = 'http://static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
  (function($){
    $('.fancybox').fancybox();
  })(jQuery);
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!--<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>--><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>